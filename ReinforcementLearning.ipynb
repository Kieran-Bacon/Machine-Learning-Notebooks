{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Reinforcement Learning </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Space for imports\n",
    "%pylab inline\n",
    "figsize(10, 8)\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld example\n",
    "\n",
    "The following code will provide you with a working gridworld problem. You are asked to study the code and use it to provide a solution in the following questions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid world:\n",
      "+-+-+-+-+-+-+\n",
      "| | | | | | |\n",
      "+-+-+-+-+-+-+\n",
      "| | | | | | |\n",
      "+-+-+-+-+-+-+\n",
      "| | | | | | |\n",
      "+-+-+-+-+-+-+\n",
      "|S|X|X|X|X|G|\n",
      "+-+-+-+-+-+-+\n",
      "\n",
      "GridWorld state indexing:\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 3.  2.  2.  2.  2.  1.]]\n",
      "Grid world:\n",
      "+-+-+-+-+-+-+\n",
      "|S| | | | | |\n",
      "+-+-+-+-+-+-+\n",
      "| | | | | | |\n",
      "+-+-+-+-+-+-+\n",
      "| | | | | | |\n",
      "+-+-+-+-+-+-+\n",
      "| | | | | |G|\n",
      "+-+-+-+-+-+-+\n",
      "\n",
      "[[ 0  4  8 12 16 20]\n",
      " [ 1  5  9 13 17 21]\n",
      " [ 2  6 10 14 18 22]\n",
      " [ 3  7 11 15 19 23]]\n",
      "Initial value function:\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "class GridWorld: \n",
    "    '''\n",
    "    Class implementing a typical GridWorld problem. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, width, height, gamma=1, penalty=-1, start=[0,0], goal=[0,0], cliff=[], cliff_penalty=-10):\n",
    "        '''\n",
    "        Generates a new Gridworld problem, with a grid of size (width,height). \n",
    "        gamma: reward discoount parameter (default:1)\n",
    "        penalty: penalty for each move (default:-1)\n",
    "        start: start location (default: [0,0])\n",
    "        goal: goal location (default: [0,0])\n",
    "        cliff: list of locations defined as cliff (the agent incurs a large penalty for walking into cliffs and is transported back to the start). \n",
    "        cliff_penalty: penalty incurred for walking into a cliff (default: -100). \n",
    "        '''\n",
    "        # define the four possible actions\n",
    "        self.gamma = gamma\n",
    "        self.actions = range(4)\n",
    "        self.delta = np.array([[0,-1],[+1,0],[0,+1],[-1,0]])\n",
    "        self.actions_labels = ['up','right','down','left']\n",
    "        \n",
    "        self.states = range(width*height)\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "        # set the start state\n",
    "        self.start = self.encode_state(start)     \n",
    "\n",
    "        self.cliff = [self.encode_state(s) for s in cliff]\n",
    "        self.goal = self.encode_state(goal)\n",
    "        self.penalty = penalty\n",
    "        self.cliff_penalty = cliff_penalty\n",
    "        self.grid = np.zeros((height,width))\n",
    "        for c in range(width): \n",
    "            for r in range(height): \n",
    "                if [c,r] == goal: \n",
    "                    self.grid[r,c] = 1\n",
    "                elif [c,r] in cliff: \n",
    "                    self.grid[r,c] = 2\n",
    "                elif [c,r] == start: \n",
    "                    self.grid[r,c] = 3\n",
    "                    \n",
    "    def action_coordinate(self, action):\n",
    "        if action in self.actions_labels:\n",
    "            return self.actions_labels.index(action)\n",
    "        \n",
    "    def line_string(self):\n",
    "        s = '+'\n",
    "        for j in range(self.width):\n",
    "            s += '-+'\n",
    "        s += '\\n'\n",
    "        return s\n",
    "        \n",
    "    def tostring(self): \n",
    "        '''\n",
    "            Converts the grid world to a formatted string. \n",
    "        '''\n",
    "        s = 'Grid world:\\n'\n",
    "        for r in range(self.height): \n",
    "            s += self.line_string()\n",
    "            s += '|'\n",
    "            for c in range(self.width):                 \n",
    "                if self.grid[r,c] == 1: \n",
    "                    s += 'G'\n",
    "                elif self.grid[r,c] == 2: \n",
    "                    s += 'X'\n",
    "                elif self.grid[r,c] == 3:\n",
    "                    s += 'S'\n",
    "                else: \n",
    "                    s += ' '\n",
    "                s += '|'\n",
    "            s += '\\n'\n",
    "            \n",
    "        s += self.line_string()\n",
    "        return s    \n",
    "    \n",
    "    def print_solution(self,S): \n",
    "        '''\n",
    "        Graphical display of a solution S, given as a list of states that brings from the initial state to the goal. \n",
    "        '''\n",
    "        s = 'Solution:\\n'\n",
    "        for r in range(self.height): \n",
    "            s += self.line_string()\n",
    "            s += '|'\n",
    "            for c in range(self.width):                 \n",
    "                p = self.encode_state((c,r))                \n",
    "                \n",
    "                if self.grid[r,c] == 1: \n",
    "                    s += 'G'\n",
    "                elif self.grid[r,c] == 3:\n",
    "                    s += 'S'\n",
    "                elif p in S:\n",
    "                    if p == S[-1]:\n",
    "                        s += '@'\n",
    "                    else:\n",
    "                        s += '*'\n",
    "                elif self.grid[r,c] == 2: \n",
    "                    s += 'X'\n",
    "                else: \n",
    "                    s += ' '\n",
    "                s += '|'\n",
    "            s += '\\n'\n",
    "            \n",
    "        s += self.line_string()\n",
    "        return s \n",
    "    \n",
    "    def show_states(self):\n",
    "        '''\n",
    "        Displays state IDs on a grid. \n",
    "        '''\n",
    "        print( np.array(self.states).reshape((self.width,self.height)).T )\n",
    "    \n",
    "    def encode_state(self,coord):\n",
    "        '''\n",
    "        Converts a tuple (c,r) containing grid coordinates to the corresponding (scalar) state ID. \n",
    "        '''\n",
    "        (c,r) = coord\n",
    "        if r<self.height and c<self.width and r>=0 and c>=0: \n",
    "            return int(c*self.height+r)\n",
    "        else: \n",
    "            return -1\n",
    "    \n",
    "    def decode_state(self,s): \n",
    "        '''\n",
    "        Converts a state ID into grid coordinates. \n",
    "        '''\n",
    "        return (s//self.height,s%self.height)\n",
    "        \n",
    "    def state_action_state(self,s,a): \n",
    "        '''\n",
    "        Generates the next state and associated reward for a state/action pair. \n",
    "        '''\n",
    "        \n",
    "        ns = self.encode_state(self.decode_state(s) + self.delta[a,:])\n",
    "        \n",
    "        # If you hit a cliff, go back to the start! Suffer the cliff penalty\n",
    "        if ns in self.cliff: \n",
    "            return (self.start, self.cliff_penalty)\n",
    "        \n",
    "        # if an invalid state is generated, stay where we are\n",
    "        elif ns < 0: \n",
    "            return (s,self.penalty)\n",
    "        \n",
    "        # if the goal is reached, no penalty incurred. \n",
    "        elif ns == self.goal: \n",
    "            return (ns,0)\n",
    "        \n",
    "        else: \n",
    "            return (ns,self.penalty)\n",
    "        \n",
    "    def state_transition(self,s): \n",
    "        '''\n",
    "        Generates lists of all possible action/next state/reward triplets for the current state. \n",
    "        Returns a triplet of lists (sv,pv,rv), with one element per action, where: \n",
    "        sv[a] is the next state following the action a.\n",
    "        pv[a] is the likelihood to select action a (policy). \n",
    "        rv[a] is the immediate reward of selecting action a. \n",
    "        '''\n",
    "        \n",
    "        sv = np.zeros(len(self.actions),dtype='uint')\n",
    "        #pv = np.zeros(len(self.actions))\n",
    "        rv = np.zeros(len(self.actions))\n",
    "        \n",
    "        for a in self.actions: \n",
    "            (s2,r2) = self.state_action_state(s,a)\n",
    "            sv[a] = s2\n",
    "            #pv[a] = 1.0/len(self.actions)\n",
    "            rv[a] = r2\n",
    "            \n",
    "        return (sv,rv)\n",
    "            \n",
    "world = GridWorld(6,4, gamma=0.9, goal=[5,3], start=[0,3], cliff=[[1,3],[2,3],[3,3],[4,3]])\n",
    "print( world.tostring() )\n",
    "world.state_action_state(2,0)\n",
    "\n",
    "print('GridWorld state indexing:')\n",
    "print(world.grid)\n",
    "\n",
    "world = GridWorld(6,4, gamma=0.9, start=[0,0], goal=[5,3])\n",
    "print(world.tostring())\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "world.show_states()\n",
    "\n",
    "print('Initial value function:')\n",
    "V = np.zeros(len(world.states))\n",
    "print(V.reshape((world.width,world.height)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\" Representation of a agents decision making process \"\"\"\n",
    "    \n",
    "    def __init__(self, world: GridWorld, V_function = None, Q_function = None, optimiser=None, epsilon=0.2):\n",
    "        \"\"\" Initialise the policy variables \n",
    "        \n",
    "        Params:\n",
    "            world - The environment in which the agent acts\n",
    "            V_function - State values for the environment states\n",
    "            Q_function - State and action values that can be taken in the environment \n",
    "            epsilon - probability of following none value orientated paths\n",
    "        \"\"\"\n",
    "        self.world = world\n",
    "        \n",
    "        self.V = V_function\n",
    "        if optimiser == \"V\":\n",
    "            self.V = {s:0 for s in world.states}\n",
    "        \n",
    "        self.Q = Q_function\n",
    "        if optimiser == \"Q\":\n",
    "            self.Q = {}\n",
    "            for state in self.world.states:\n",
    "                self.Q[state] = {a: 0.0 for s, a in zip(self.world.state_transition(state)[0], self.world.actions_labels) if s != state}\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def run(self) -> [int]:\n",
    "        \"\"\" Return the agents solution to the problem\n",
    "        \n",
    "        Returns:\n",
    "            [int] - A list of state id's indicating a path from state to state\n",
    "        \"\"\"\n",
    "        state = self.world.start # Start the agent in the problems starting position\n",
    "        path = [state]           # Record agents path\n",
    "        \n",
    "        while state != self.world.goal:\n",
    "            \n",
    "            # Navigate the path through value towards the goal\n",
    "            action = self.decision(state)\n",
    "            state, _ = self.world.state_action_state(state, self.world.action_coordinate(action))\n",
    "            path.append(state)\n",
    "            \n",
    "        return path # Return the solution path\n",
    "    \n",
    "    def valueMove(self, state: int) -> int:\n",
    "        \"\"\" For a given state, return the state as dictated by the state with highest value \n",
    "        \n",
    "        Params:\n",
    "            state - The id of the initial state of the agent\n",
    "        \n",
    "        Returns:\n",
    "            int - The state id of the adjacent state with the highest value\n",
    "        \"\"\"\n",
    "        states, values = self.nextStatesValues(state)\n",
    "        return states[Policy._max(values)]\n",
    "        \n",
    "    def action(self, state: int, _state: int) -> str:\n",
    "        \"\"\" Given back the action responsible for moving from one state to another \n",
    "        \n",
    "        Params:\n",
    "            state - Initial position state\n",
    "            _state - The adjacent state\n",
    "            \n",
    "        Returns:\n",
    "            str - The action label responsible for the move \n",
    "            \n",
    "        Raises:\n",
    "            Error - _state is not an adjacent state\n",
    "        \"\"\"\n",
    "        states, _ = self.world.state_transition(state)\n",
    "        if _state not in states: raise Error(\"No action moves from \" + str(state) + \" to \" + str(_state))\n",
    "        return self.world.actions_labels[list(states).index(_state)]\n",
    "        \n",
    "    def probAction(self, state: int, _state: int) -> float:\n",
    "        \"\"\" Calculate the probability of being in a state and moving into an adjacent state\n",
    "        \n",
    "        Params:\n",
    "            state - Initial state of the agent\n",
    "            _state - The adjacent state the action transitions too\n",
    "            \n",
    "        Returns:\n",
    "            float - The probability of such a move\n",
    "        \"\"\"\n",
    "        return {stateId: probState for stateId, probState in self.probDecision(state)}[_state]\n",
    "    \n",
    "    def nextStates(self, state: int) -> [int]:\n",
    "        \"\"\" Return a list of the possible adjacent states the policy would want to move too \"\"\"\n",
    "        return self.nextStatesValues(state)[0]\n",
    "    \n",
    "    def nextStatesValues(self, state: int) -> ([int], [float]):\n",
    "        \"\"\" Return the next possible states and their values for a \n",
    "        given state, for V or Q optimisation\n",
    "        \n",
    "        Params:\n",
    "            state - The id of the state who's adjacent states are desired\n",
    "            \n",
    "        Returns:\n",
    "            int - A collection of state ids of the adjacent states\n",
    "            float - Their respective value in the policy\n",
    "            \n",
    "        Throws:\n",
    "            Error - If neither a V or Q has been set values cannot be extracted\n",
    "        \"\"\"\n",
    "        states, _ = self.world.state_transition(state)\n",
    "        \n",
    "        if self.V is not None:\n",
    "            states = [s for s in states if s != state]    # Ensure not moving to current state\n",
    "            return states, [self.V[s] for s in states]\n",
    "        \n",
    "        if self.Q is not None:\n",
    "            Qs = [self.Q[state][action] for i, action in enumerate(self.world.actions_labels) if states[i] != state]\n",
    "            return [s for s in states if s != state], Qs \n",
    "        \n",
    "        raise Error(str(type(self)) + \" has no V or Q function when attempting to find nextStatesValues\")\n",
    "        \n",
    "    def _max(values: [float]) -> int:\n",
    "        \"\"\" Return the highest valued element of the list, randomly select from the\n",
    "        the highest value if multiple values are equal.\n",
    "        \n",
    "        Params:\n",
    "            state - List of values representing state worth\n",
    "            \n",
    "        Returns:\n",
    "            int - The index of the choosen value in the list\n",
    "        \"\"\"\n",
    "        return random.choice([i for i in range(len(values)) if values[i] == max(values)])\n",
    "    \n",
    "    def Vrepr(self):\n",
    "        \"\"\" Transform the V function into a format to that of the environment \"\"\"\n",
    "        return array([self.V[s] for s in self.world.states]).reshape((self.world.width, self.world.height)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GreedyPolicy(Policy):\n",
    "    \"\"\" Policy of moving entirely towards highest value at all times \"\"\"\n",
    "    \n",
    "    def decision(self, state: int) -> str:\n",
    "        \"\"\" The agent's action decision \n",
    "        \n",
    "        Params:\n",
    "            state - The state id of the agent's initial location\n",
    "            \n",
    "        Returns:\n",
    "            str - The action label of the chosen action\n",
    "        \"\"\"\n",
    "        return self.action(state, self.valueMove(state))\n",
    "        \n",
    "    \n",
    "    def probDecision(self, state: int) -> (int, float):\n",
    "        \"\"\" The probability associated with avaliable actions the agent can take\n",
    "        \n",
    "        Params:\n",
    "            state - The state if of the agent's initial location\n",
    "            \n",
    "        Returns:\n",
    "            (int, float) - State probability combinations of policy thought\n",
    "        \"\"\"\n",
    "        states, values = self.nextStatesValues(state)\n",
    "        maxValue = max(values)\n",
    "        maxCount = len([1 for v in values if v == maxValue]) # Count numbers of equal highest valued states\n",
    "        \n",
    "        # Generate list of probabilities\n",
    "        for i, v in enumerate(values):\n",
    "            values[i] = 0 if v != maxValue else 1/maxCount\n",
    "        \n",
    "        # Return\n",
    "        return zip(states, values)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(Policy):\n",
    "    \"\"\" Policy: Epsilon-Greedy, alternate between greedy and exploritive \"\"\"\n",
    "    \n",
    "    def decision(self, state: int) -> int:\n",
    "        \"\"\" Determine what adjacent state the agent should transition into, given\n",
    "        a current state.\n",
    "        \n",
    "        Params:\n",
    "            state - State id of the current state\n",
    "            \n",
    "        Returns:\n",
    "            int - The state id of the adjacent state the agent choose\n",
    "        \"\"\"\n",
    "        states, values = self.nextStatesValues(state)\n",
    "        \n",
    "        if random.random() > self.epsilon:\n",
    "            # Choose state with greatest value\n",
    "            return self.action(state,states[Policy._max(values)])\n",
    "        \n",
    "        else:\n",
    "            # Explore the alternate states\n",
    "            del states[Policy._max(values)]\n",
    "            return self.action(state,states[random.randint(0,len(states)-1)])\n",
    "        \n",
    "    def probDecision(self, state: int) -> [(int,float)]:\n",
    "        \"\"\" Determine the probabilty for each adjacent state, that hee policy might select it\n",
    "        \n",
    "        Params:\n",
    "            state - The state id of the agent location\n",
    "            \n",
    "        Returns:\n",
    "            [(int,float)] - List of state id - probability pairs.\n",
    "        \"\"\"\n",
    "        \n",
    "        states, values = self.nextStatesValues(state)\n",
    "\n",
    "        # Identify the greatest valued adjacent states\n",
    "        max_indexes = [i for i in range(len(values)) if values[i] == max(values)]\n",
    "        if len(states) == len(max_indexes):\n",
    "            # All surrounding states have equal likelihood of being choosen as their value is equal\n",
    "            return zip(states, [1/len(states)]*len(states))\n",
    "        \n",
    "        # Calculate the probabilites for the maximum value state and others.\n",
    "        prob = [1/(len(states)-len(max_indexes)) * self.epsilon]*len(states)\n",
    "        for i in max_indexes:\n",
    "            prob[i] = 1/(len(max_indexes)) * (1 - self.epsilon)\n",
    "        \n",
    "        return zip(states, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExplorativePolicy(Policy):\n",
    "    \"\"\" Policy: State selection is a function of its value over total value\"\"\"\n",
    "    \n",
    "    def decision(self, state: int) -> int:\n",
    "        \"\"\" Determine what adjacent state the agent should transition into, given\n",
    "        a current state.\n",
    "        \n",
    "        Params:\n",
    "            state - State id of the current state\n",
    "            \n",
    "        Returns:\n",
    "            int - The state id of the adjacent state the agent choose\n",
    "        \"\"\"\n",
    "        \n",
    "        select, prob = random.random(), 0.0 # Collect probabilites for next states\n",
    "        \n",
    "        # Iterate through probabilities until random selection occurs\n",
    "        for nextState, probability in self.probDecision(state):\n",
    "            prob += probability\n",
    "            if select <= prob:\n",
    "                return self.action(state,nextState)\n",
    "            \n",
    "    def probDecision(self, state: int) -> int:\n",
    "        \"\"\" Give probability of state selection in respect to the amount of total value available. \n",
    "        States with 0 value are explored first. \n",
    "        \n",
    "        Params:\n",
    "            state - The state id of the agent location\n",
    "            \n",
    "        Returns:\n",
    "            [(int,float)] - List of state id - probability pairs\n",
    "            \"\"\"\n",
    "        \n",
    "        states, values = self.nextStatesValues(state)\n",
    "        \n",
    "        if 0 in values or 0.0 in values:\n",
    "            count = values.count(0)\n",
    "            for i, v in enumerate(values):\n",
    "                values[i] = 0 if v != 0 else 1/count\n",
    "        else:\n",
    "            totalValue = abs(sum(values))\n",
    "            for i, v in enumerate(values):\n",
    "                values[i] = 1-(abs(v)/totalValue)\n",
    "            values = [v/sum(values) for v in values]\n",
    "                \n",
    "        return zip(states, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policyIteration(world: GridWorld, policy: Policy) -> Policy:\n",
    "    \"\"\" The dynamical programming implementation of the policy iteration function, it takes \n",
    "    an initial policy and continues to evaluate and improve said policy until no inconsistances appear \n",
    "    between newer iterations of the policy and the current policy.\n",
    "    \n",
    "    Params:\n",
    "        world - The environment in which the policy is acting\n",
    "        policy - The policy subject to improvement\n",
    "        \n",
    "    Returns:\n",
    "        Policy - A policy that does would remain consistent even after the evaluation function is acted.\n",
    "    \"\"\"\n",
    "    \n",
    "    iterations = 0 # Iteration counter\n",
    "    while True:\n",
    "        \n",
    "        if iterations in [1,2,5,10]:\n",
    "            print(\"Policy V function values after\", iterations, \"iterations.\")\n",
    "            print(policy.Vrepr())\n",
    "        \n",
    "        stable = True                                  # Check on similarity of old and new policy\n",
    "        newPolicy = type(policy)(world, policy.V)      # Create new policy with old policy's V function\n",
    "        newPolicy = policyEvaluation(world, newPolicy) # Evaluate the policy and return its improvement\n",
    "        \n",
    "        if policy.V != newPolicy.V:\n",
    "            for state in world.states:\n",
    "                # For every state check old and new policy align on decision movement\n",
    "                if policy.valueMove(state) != newPolicy.valueMove(state):\n",
    "                    stable = False # The policies differ, therefore further improvement is necessary\n",
    "        \n",
    "        iterations += 1\n",
    "        if stable:\n",
    "            print(\"System stabalised in\", iterations, \"iterations.\")\n",
    "            return policy\n",
    "        else:\n",
    "            # Replace the old policy with the improved varient\n",
    "            policy = newPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policyEvaluation(world: GridWorld, policy: Policy, epsilon=0.01, show=False, printer=False) -> Policy:\n",
    "    \"\"\" Dynamic Programming's policy evaluation method, calculate values for the world states \n",
    "    from the agent's policy.\n",
    "    \n",
    "    Params:\n",
    "        world - The environment in which the agent acts\n",
    "        policy - The policy the agent used to navigate the environment\n",
    "        epsilon - The learning threshold, signals to stop when iterations provide no improvement.\n",
    "        \n",
    "    Returns:\n",
    "        Policy - The initial policy with updated state value scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    def recursiveBellman(state: int) -> int:\n",
    "        \"\"\" Recursive Bellman equation implementation to evauluate a particular state of the world\n",
    "        given the policy.\n",
    "        \n",
    "        Params:\n",
    "            state - integer value representing the current state\n",
    "        \n",
    "        Returns:\n",
    "            int - New calculated value of the original state\n",
    "        \"\"\"\n",
    "        if state == world.goal: \n",
    "            return 0 # Reached goal\n",
    "        \n",
    "        value = 0.0 # Sum of value for state\n",
    "        for _state, _reward in zip(*world.state_transition(state)):\n",
    "            if _state == state: continue # Do not include consider non-moving actions\n",
    "            value += round(policy.probAction(state,_state)*(_reward + world.gamma*policy.V[_state]), 10)\n",
    "        return value #/len(policy.nextStates(state)) #Provides greater focus on goal\n",
    "    \n",
    "    iterations = 0\n",
    "    while not printer or (iterations <= 10):\n",
    "        if printer and iterations in [1,2,5,10]:\n",
    "            print(\"Policy Evaluations: After\", iterations, \"iterations V values are :\")\n",
    "            print(policy.Vrepr())\n",
    "        delta = 0 # Record of the largest change of state value in each iteration.\n",
    "        V_current = policy.V.copy() # Create a new copy of the policy's V function for updates\n",
    "        \n",
    "        for state in world.states:\n",
    "            currentValue = V_current[state]            \n",
    "            V_current[state] = recursiveBellman(state)\n",
    "            delta = max(delta, abs(currentValue - V_current[state])) # Record largest value change\n",
    "            \n",
    "        policy.V = V_current.copy() # Update the policy with the new improvement\n",
    "        iterations += 1\n",
    "        \n",
    "        if (delta < epsilon) and not printer:\n",
    "            if show: print(\"Process ended after\", iterations, \"iterations\")\n",
    "            break # End loop when delta is sufficiently small\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strictIteration(world: GridWorld, V: [float]) -> [float]:\n",
    "    V = policyEvaluation(world, GreedyPolicy(world, V_function={s:v for s,v in enumerate(V)})).V\n",
    "    return [V[i] for i in range(len(world.states))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy iteration\n",
    "\n",
    "One way to solve a reinforcement learning problem is to estimate the value function for the whole problem and apply a greedy strategy to find the goal. In the first part you are asked to evaluate the value function V(s) for all states (ie, grid cells), using policy iteration as discussed in class. \n",
    "\n",
    "a. Estimate the value of V(s) using policy iteration. Each step of policy iteration will take as parameter the problem to be solved (which will provide the required functions) and the current estimate of the value function (as an array with length equal to the grid size). \n",
    "\n",
    "b. Print out the estimated value function after 1,2,5 and 10 iterations. \n",
    "\n",
    "c. Provide a solution using a greedy strategy and the estimated value function. Print it out using the print_solution() method.\n",
    "\n",
    "Criteria\n",
    "- Does it record what was done in the exercise?\n",
    "- Does it permit the results to be reproduced?\n",
    "- How does the work relate to the theoretical foundations discussed in lectures?\n",
    "- Is it well presented?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.a)\n",
    "\n",
    "Policy iteration as described in the lecture notes is the combination of two policy improvement functions. The first of which is the policy iteration sweep (implemented as policyIteration), that aims to improve a policy by using an arbitrary value improvement function until decisions occur determinisitcally from one iteration to the next. The second is the policy evaluation sweep (implemented as policyEvalutation), that implements the iterative policy evalutation function to generate a new Value function for the policy from changes its initial Values.\n",
    "\n",
    "It is my intension to use this method with a number of policies and have therefore made its contract take both the environment, and a policy object. To conform to the contract, the function strictIteration is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.b)\n",
    "\n",
    "The iterative policy proceedure is able to come to a stable policy by the second iteration, which implies that this method is realistically not necessary for improving the final value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy V function values after 1 iterations.\n",
      "[[-5.22 -4.69 -4.1  -3.44 -2.71 -1.9 ]\n",
      " [-4.69 -4.1  -3.44 -2.71 -1.9  -1.  ]\n",
      " [-4.1  -3.44 -2.71 -1.9  -1.    0.  ]\n",
      " [-3.44 -2.71 -1.9  -1.    0.    0.  ]]\n",
      "System stabalised in 2 iterations.\n",
      "The V function of the final policy\n",
      "[[-5.22 -4.69 -4.1  -3.44 -2.71 -1.9 ]\n",
      " [-4.69 -4.1  -3.44 -2.71 -1.9  -1.  ]\n",
      " [-4.1  -3.44 -2.71 -1.9  -1.    0.  ]\n",
      " [-3.44 -2.71 -1.9  -1.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "world = GridWorld(6,4, gamma=0.9, start=[0,0], goal=[5,3])\n",
    "policy = policyIteration(world, GreedyPolicy(world, optimiser=\"V\"))\n",
    "\n",
    "print(\"The V function of the final policy\")\n",
    "print(policy.Vrepr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilising the policy evaluation only to look at the values generated, we are able to learn the intrinsic nature of the GridWorld in 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluations: After 1 iterations V values are :\n",
      "[[-1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      " [-1.   -1.   -1.   -1.   -1.   -1.  ]\n",
      " [-1.   -1.   -1.   -1.   -1.   -0.67]\n",
      " [-1.   -1.   -1.   -1.   -0.67  0.  ]]\n",
      "Policy Evaluations: After 2 iterations V values are :\n",
      "[[-1.9 -1.9 -1.9 -1.9 -1.9 -1.9]\n",
      " [-1.9 -1.9 -1.9 -1.9 -1.9 -1.6]\n",
      " [-1.9 -1.9 -1.9 -1.9 -1.6  0. ]\n",
      " [-1.9 -1.9 -1.9 -1.6  0.   0. ]]\n",
      "Policy Evaluations: After 5 iterations V values are :\n",
      "[[-4.1  -4.1  -4.1  -3.88 -2.71 -1.9 ]\n",
      " [-4.1  -4.1  -3.88 -2.71 -1.9  -1.  ]\n",
      " [-4.1  -3.88 -2.71 -1.9  -1.    0.  ]\n",
      " [-3.88 -2.71 -1.9  -1.    0.    0.  ]]\n",
      "Policy Evaluations: After 10 iterations V values are :\n",
      "[[-5.22 -4.69 -4.1  -3.44 -2.71 -1.9 ]\n",
      " [-4.69 -4.1  -3.44 -2.71 -1.9  -1.  ]\n",
      " [-4.1  -3.44 -2.71 -1.9  -1.    0.  ]\n",
      " [-3.44 -2.71 -1.9  -1.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "world = GridWorld(6,4, gamma=0.9, start=[0,0], goal=[5,3])\n",
    "policy = policyEvaluation(world, GreedyPolicy(world, optimiser=\"V\"), printer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.c)\n",
    "\n",
    "Provided is a single instance of the method using the policy evaluation technique, and the result it was able to achieve.\n",
    "\n",
    "The agent has learnt that all down and right combinations are acceptable and equally likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration solution for grid of dimensions (6, 4)\n",
      "Process ended after 10 iterations\n",
      "[[-5.22 -4.69 -4.1  -3.44 -2.71 -1.9 ]\n",
      " [-4.69 -4.1  -3.44 -2.71 -1.9  -1.  ]\n",
      " [-4.1  -3.44 -2.71 -1.9  -1.    0.  ]\n",
      " [-3.44 -2.71 -1.9  -1.    0.    0.  ]]\n",
      "Solution:\n",
      "+-+-+-+-+-+-+\n",
      "|S| | | | | |\n",
      "+-+-+-+-+-+-+\n",
      "|*|*| | | | |\n",
      "+-+-+-+-+-+-+\n",
      "| |*| | | | |\n",
      "+-+-+-+-+-+-+\n",
      "| |*|*|*|*|G|\n",
      "+-+-+-+-+-+-+\n",
      "\n",
      "Policy iteration solution for grid of dimensions (6, 6)\n",
      "Process ended after 12 iterations\n",
      "[[-6.13 -5.7  -5.22 -4.69 -4.1  -3.44]\n",
      " [-5.7  -5.22 -4.69 -4.1  -3.44 -2.71]\n",
      " [-5.22 -4.69 -4.1  -3.44 -2.71 -1.9 ]\n",
      " [-4.69 -4.1  -3.44 -2.71 -1.9  -1.  ]\n",
      " [-4.1  -3.44 -2.71 -1.9  -1.    0.  ]\n",
      " [-3.44 -2.71 -1.9  -1.    0.    0.  ]]\n",
      "Solution:\n",
      "+-+-+-+-+-+-+\n",
      "|S| | | | | |\n",
      "+-+-+-+-+-+-+\n",
      "|*| | | | | |\n",
      "+-+-+-+-+-+-+\n",
      "|*|*| | | | |\n",
      "+-+-+-+-+-+-+\n",
      "| |*|*|*| | |\n",
      "+-+-+-+-+-+-+\n",
      "| | | |*| | |\n",
      "+-+-+-+-+-+-+\n",
      "| | | |*|*|G|\n",
      "+-+-+-+-+-+-+\n",
      "\n",
      "Policy iteration solution for grid of dimensions (4, 6)\n",
      "Process ended after 10 iterations\n",
      "[[-5.22 -4.69 -4.1  -3.44]\n",
      " [-4.69 -4.1  -3.44 -2.71]\n",
      " [-4.1  -3.44 -2.71 -1.9 ]\n",
      " [-3.44 -2.71 -1.9  -1.  ]\n",
      " [-2.71 -1.9  -1.    0.  ]\n",
      " [-1.9  -1.    0.    0.  ]]\n",
      "Solution:\n",
      "+-+-+-+-+\n",
      "|S| | | |\n",
      "+-+-+-+-+\n",
      "|*| | | |\n",
      "+-+-+-+-+\n",
      "|*|*| | |\n",
      "+-+-+-+-+\n",
      "| |*|*|*|\n",
      "+-+-+-+-+\n",
      "| | | |*|\n",
      "+-+-+-+-+\n",
      "| | | |G|\n",
      "+-+-+-+-+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display multiple grid sizes and goals\n",
    "for dimensions, goal in [((6,4),[5,3]), ((6,6),[5,5]), ((4,6),[3,5])]:\n",
    "    w, h = dimensions # Open dimensions of the grid\n",
    "    print(\"Policy iteration solution for grid of dimensions (\" + str(w) + \", \" + str(h) +\")\")\n",
    "    \n",
    "    world = GridWorld(w,h, gamma=0.9, start=[0,0], goal=goal) # Create the Grid world problem\n",
    "    policy = GreedyPolicy(world, optimiser=\"V\")               # Create the agent's policy\n",
    "    policy = policyEvaluation(world, policy, show=True)       # Begin optimising the policies V function\n",
    "    \n",
    "    # Print results\n",
    "    print(policy.Vrepr())\n",
    "    print(world.print_solution(policy.run()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State-Action-Reward-State-Action (SARSA)\n",
    "In this second part, you are asked to implement the on-policy SARSA algorithm to the problem. Choose an $\\epsilon$-greedy policy. Finally, show the solution your algorithm has reached for different values of $\\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SARSA(environment: GridWorld, policy: Policy, episodes=500, alpha=0.1) -> Policy:\n",
    "    \"\"\" State action reward state action reinforcement algorithm \n",
    "    \n",
    "    Params:\n",
    "        environment - The environment in which the agent operates\n",
    "        policy - The policy the agent employs to move around the environment\n",
    "        episodes - The number of iterations of the environment the agent can tranverse\n",
    "                   before computation stops\n",
    "        alpha - The learning rate applyied of the next state actions\n",
    "        \n",
    "    Returns:\n",
    "        Policy - A new policy with an improved Q function \n",
    "    \"\"\"\n",
    "    for cycle in range(episodes):\n",
    "        \n",
    "        policy.time = 0.1                      # Reset the policy time variable\n",
    "        currentState = environment.start       # Collect the starting position\n",
    "        action = policy.decision(currentState) # Use policy to determine first action\n",
    "        \n",
    "        while currentState != environment.goal: # Continue tranversal until the goal is reached\n",
    "    \n",
    "            # Collect the next state and reward of the action\n",
    "            nextState, reward = environment.state_action_state(currentState, environment.action_coordinate(action))\n",
    "            # Given the next state, determine the subsequent action\n",
    "            nextAction = policy.decision(nextState)\n",
    "            \n",
    "            # Calculate the value of the initial state as a consequence of its value and future action\n",
    "            policy.Q[currentState][action] =\\\n",
    "                policy.Q[currentState][action] + \\\n",
    "                alpha*(reward + environment.gamma*policy.Q[nextState][nextAction] - policy.Q[currentState][action])\n",
    "            \n",
    "            # Move into the new state, and prepare to take the subsequent action\n",
    "            currentState, action = nextState, nextAction\n",
    "                        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EpsilonValues = [0.1, 0.2, 0.3] # Espilon values used for the Espilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA function with epsilon greedy policy of epsilon = 0.1 took:\n",
      "28 steps to finish\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|S| |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| | |*|*|*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*|*| | |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*| |X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*| |*|*| |X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*| | |X| |*|*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | |X| | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | |X| | |G|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "SARSA function with epsilon greedy policy of epsilon = 0.2 took:\n",
      "57 steps to finish\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|S| |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| | |*|*|*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*|*| | |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*| |X|*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | | |*| |X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|*| |X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | | |*| |X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | |X| |*|G|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "SARSA function with epsilon greedy policy of epsilon = 0.3 took:\n",
      "694 steps to finish\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|S| |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*|*|*|*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*|*|*| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X|*|*|*|X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|*| |X| |*|*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|*| |X| | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|*| |X| | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*| | |X| | |G|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world1 = GridWorld(10,10, gamma=0.9, start=[0,0], goal=[9,9], cliff=[[2,0], [2,1], [2,2], [2,3], [2,4], [2,5], [6,5], [6,6], [6,7],[6,8],[6,9]])\n",
    "\n",
    "for e in EpsilonValues:\n",
    "    \n",
    "    policy = EpsilonGreedyPolicy(world1, epsilon=e, optimiser=\"Q\")  # Create a new policy\n",
    "    policy = SARSA(world1, policy, episodes=1000)                   # Learn the knowledge from the environment\n",
    "    solution = policy.run()                                         # Run the policy\n",
    "    \n",
    "    print(\"SARSA function with epsilon greedy policy of epsilon =\", e, \"took:\")\n",
    "    print(len(solution)-1, \"steps to finish\")\n",
    "    print(world1.print_solution(solution))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA function with epsilon greedy policy of epsilon = 0.1 took:\n",
      "20 steps to finish\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|S|*|*|*|*|*|*|*| | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | |*|*| | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | |*|*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | |*|*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |G|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "SARSA function with epsilon greedy policy of epsilon = 0.2 took:\n",
      "18 steps to finish\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|S|*|*|*|*|*| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | |*|*|*| | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | |*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| |*|*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |G|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "SARSA function with epsilon greedy policy of epsilon = 0.3 took:\n",
      "28 steps to finish\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|S|*|*|*|*|*|*|*|*|*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | |*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | |*|*|*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| |*|*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |G|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cliff = [ [i,j] for i in range(3,7) for j in range(3,7) ]\n",
    "world2 = GridWorld(10,10, gamma=0.9, start=[0,0], goal=[9,9], cliff=cliff)\n",
    "\n",
    "for e in EpsilonValues:\n",
    "    \n",
    "    policy = EpsilonGreedyPolicy(world2, epsilon=e, optimiser=\"Q\")  # Create a new policy\n",
    "    policy = SARSA(world2, policy, episodes=1000)                   # Learn the knowledge from the environment\n",
    "    solution = policy.run()                                         # Run the policy\n",
    "    \n",
    "    print(\"SARSA function with epsilon greedy policy of epsilon =\", e, \"took:\")\n",
    "    print(len(solution)-1, \"steps to finish\")\n",
    "    print(world2.print_solution(solution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "environments = [world1, world2]\n",
    "iterations = linspace(350,900,10)\n",
    "\n",
    "worlds = []\n",
    "for env in environments: # For each environment\n",
    "    epsValues = []\n",
    "    for e in EpsilonValues: # For each epsilon value\n",
    "        solution = []\n",
    "        for i in iterations: # For each number of episodes\n",
    "            # Produce a Epsilon-Greedy via the SARSA method\n",
    "            policy = EpsilonGreedyPolicy(env, epsilon=e, optimiser=\"Q\")\n",
    "            policy = SARSA(env, policy, episodes=int(i))\n",
    "            solution.append(len(policy.run()))\n",
    "        epsValues.append(solution)\n",
    "    worlds.append(epsValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of solutions against episodes of function\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAAFICAYAAACxw3TQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIIgCIKQB/8fJICTK9AqCG0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac47540e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colour = ['b-','r-','y-','g-']\n",
    "print(\"The number of solutions against episodes of function\")\n",
    "figure()\n",
    "for i, env in enumerate(worlds):\n",
    "\n",
    "    subplot(221 + i)\n",
    "    for i, epsilonSolutions in enumerate(env):\n",
    "        plot(list(iterations), epsilonSolutions, colour[i], label=\"SARSA with e-greedy \" + str(EpsilonValues[i]))\n",
    "    legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "I found that smaller values of epsilon caused the agent to be initially extremely explorative. It is an aspect of this problem that the initialised state carries the highest possible value, and as such, value-driven policies tend to explore initially before tending towards the goal.\n",
    "\n",
    "For the first 300 episodes, the agent tends to take far more steps than policies with greater epsilon values. However, as the agent has less of a random element to its decision process, after that point it tends to find shorter paths that its rivals. \n",
    "\n",
    "This behaviour is a by-product of the negative reward associated with a transition, less travelled routes tend to have a higher value, and the knowledge of the goal point must propagate through many different combinations of actions as a consequence of the explorative nature.\n",
    "\n",
    "Agents with higher epsilon values have a higher tendency to choose paths already traversed (alternative choice excludes the highest value from possible next steps), this can have very good and very bad effects on the productivity of the agent. With fewer episodes of learning, the agent is reasonably capable of navigating to the goal, however, it is very unlikely for it to ever find an optimal path, as the random component tends to work counter to the knowledge learnt.\n",
    "\n",
    "We can see that the SARSA has a tendency to avoid being close to the cliff faces, seemingly acting precarious. This stems from the look ahead aspect of the method, the values of these moves are higher due to the potential to alternatively not follow value and transition into the cliff face. This characteristic, in contrast, is not found in Q-learning as it would assume that the agent does not take the cliff action. This is to its detriment as the agent's move are still determined by its policy, and this can still occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-learning \n",
    "Finally, implement the off-policy Q-learning algorithm and use it to solve the problem. Compare the two algorithms on a number of gridworld configurations and discuss the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def QLearning(world: GridWorld, policy: Policy, episodes=1000, alpha=0.1):\n",
    "    \"\"\" Implement the off policy Q-Learning technique to solve the Grid world problem \n",
    "    \n",
    "    Params:\n",
    "        world - The environment in which the agent acts\n",
    "        policy - The agent's decision policy\n",
    "        episodes - The number of runthroughs the agent has on the board\n",
    "        alpha - The limiting factor on value change\n",
    "        \n",
    "    Returns:\n",
    "        Policy - The initial policy with an updated and improved Q function\n",
    "    \"\"\"\n",
    "    \n",
    "    for cycle in range(episodes):\n",
    "        \n",
    "        state = world.start       # Start each round on the starting position\n",
    "\n",
    "        while state != world.goal: # Continue tranversal until the goal is reached\n",
    "            \n",
    "            # Use policy to determine action\n",
    "            action = policy.decision(state)\n",
    "            # Collect the next state and reward of the action\n",
    "            nextState, reward = world.state_action_state(state, world.action_coordinate(action))\n",
    "            \n",
    "            Qvalue = policy.Q[state][action]\n",
    "            maxQvalue = max(policy.Q[nextState].values())\n",
    "            \n",
    "            # Calculate the value of the initial state as a consequence of its value and future action\n",
    "            policy.Q[state][action] = Qvalue + alpha*(reward + world.gamma*maxQvalue - Qvalue)\n",
    "            \n",
    "            # Move into the new state, and prepare to take the subsequent action\n",
    "            state = nextState\n",
    "            \n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning function with epsilon greedy policy of epsilon = 0.1 took:\n",
      "20 steps to finish\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|S|*|*| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | |*|*|*| | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | |*|*|*|*| | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X|*| | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X|*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X|X|X|X| |*|*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |*|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |G|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "Q-learning function with epsilon greedy policy of epsilon = 0.2 took:\n",
      "26 steps to finish\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|S| | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | |X|X|X|X| | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | |X|X|X|X| | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | |X|X|X|X| | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | |X|X|X|X| | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*| | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| |*|*|*| | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |*|*|*|*|*|*|G|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "Q-learning function with epsilon greedy policy of epsilon = 0.3 took:\n",
      "33 steps to finish\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|S|*|*| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| |*|*|*| | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | | |*|*|*| | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | |X|X|X|X| | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | |X|X|X|X| | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*| | |X|X|X|X| | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*| |X|X|X|X| | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "| |*| | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*| | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|*|*|*|*|*|G|\n",
      "+-+-+-+-+-+-+-+-+-+-+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the grid world problem to solve\n",
    "cliff = [[i,j] for i in range(3,7) for j in range(3,7)]\n",
    "world2 = GridWorld(10,10, gamma=0.9, start=[0,0], goal=[9,9], cliff=cliff)\n",
    "\n",
    "# For each of the epsilon values, learn the environment and plot the learnt path\n",
    "for epsilon in [0.1, 0.2, 0.3]:\n",
    "    policy = QLearning(world2, EpsilonGreedyPolicy(world2, epsilon=epsilon, optimiser=\"Q\"), episodes=1000)\n",
    "    solution = policy.run()\n",
    "    print(\"Q-learning function with epsilon greedy policy of epsilon =\", epsilon, \"took:\")\n",
    "    print(len(solution)-1, \"steps to finish\")\n",
    "    print(world2.print_solution(solution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "For the Q Learning function, world 1 posed a computationally infeasible problem. It took an unbelieveable amount of time to complete a single episode and thus, could not generate a promising Q function for comparison.\n",
    "\n",
    "However, when comparing outputs of the second world implementation with the Sarsa process, the Q learning function would seem to generate answers extremely quickly and extremely accurately for identical implementations of the epsilon greedy policy. Additionally the Q-Learning technique can be seen to find shorter but risker paths selecting paths that are adjacent to cliff faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method and policy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid world:\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| |S| | | | | |X| | | | | |G| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "Grid world:\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | |X| | | |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | |X| | | |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | |X| |S| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | |X| | | |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | |X|X|X|X|X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | | | |G| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "Grid world:\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | |X|X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "|X|X|S| | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | |X| |X| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | |X| | |X| | |X| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | |X|X| | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | |X| | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| |X| | | | | | | |X| | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | |X| | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | |X| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X| | | | | | | | | |G| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | |X| | |X| | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Map design and generation\n",
    "\n",
    "# Simplistic map with a single barrier\n",
    "shield = GridWorld(15,5, gamma=0.9, start=[1,2], goal=[13,2], cliff=[[7,1+i] for i in range(3)] )\n",
    "print(shield.tostring())\n",
    "\n",
    "# Complicated map with a surrounding barrier\n",
    "horseCliff = [[2+i,6] for i in range(5)] + [[2,2+i] for i in range(4)] + [[6,2+i] for i in range(4)]\n",
    "horseShoe = GridWorld(9,9, gamma=0.9, start=[4,4], goal=[4,7], cliff=horseCliff)\n",
    "print(horseShoe.tostring())\n",
    "\n",
    "\"\"\" # Used to generate a random field of mines for the third board.\n",
    "mines = []\n",
    "for mine in range(20):\n",
    "    mines.append([round(random.random()*15), round(random.random()*15)])\n",
    "print(mines)\n",
    "\"\"\"\n",
    "\n",
    "# Erratic map, relatively simple but noticable larger.\n",
    "minefield = GridWorld(15, 15, gamma=0.9, start=[2,2], goal=[13,13], cliff=[[13, 3], [7, 5], [11, 3], [9, 9], [1, 9], [2, 10], [10, 11], [4, 4], [3, 13], [6, 1], [0, 2], [7, 1], [12, 11], [5, 14], [10, 4], [1, 2], [8, 5], [8, 14], [6, 8], [7, 4]])\n",
    "print(minefield.tostring())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of policy on method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MXIIIXIIIXIII"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "mapScores = []\n",
    "for gridmap in [shield,horseShoe, minefield]:\n",
    "    print(\"M\", end=\"\")\n",
    "    methodScores = []\n",
    "    for method in [(policyEvaluation, \"V\"), (SARSA, \"Q\"), (QLearning, \"Q\")]:\n",
    "        print(\"X\", end=\"\")\n",
    "        pmethodScores = []\n",
    "        for policyMethod in [GreedyPolicy, EpsilonGreedyPolicy, ExplorativePolicy]:\n",
    "            print(\"I\", end=\"\")\n",
    "\n",
    "            start = time.time()\n",
    "            \n",
    "            if (gridmap in [shield, horseShoe,minefield] and method[0] == SARSA and policyMethod == GreedyPolicy) or \\\n",
    "               (gridmap in [horseShoe, minefield] and method[0] == QLearning) or \\\n",
    "               (gridmap == minefield and method[0] == SARSA and policyMethod == ExplorativePolicy):\n",
    "                solution = []\n",
    "            else:\n",
    "                policy = policyMethod(gridmap, epsilon=0.1, optimiser=method[1])\n",
    "                policy = method[0](gridmap, policy)\n",
    "                solution = policy.run()\n",
    "\n",
    "            finish = time.time() - start\n",
    "            pmethodScores.append((finish, len(solution), solution))\n",
    "        methodScores.append(pmethodScores)   \n",
    "    mapScores.append(methodScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Shield\n",
      "========\n",
      "Path Length\tGreedy\t\tEpsilon\t\tExplorative\n",
      "------------------------------------------------------------\n",
      "Evaluation\t17 \t\t21 \t\t4501\n",
      "SARSA\t\t0 \t\t97 \t\t1247\n",
      "QLearning\t17 \t\t32 \t\t128\n",
      "\n",
      "Time\t\tGreedy\t\tEpsilon\t\tExplorative\n",
      "------------------------------------------------------------\n",
      "Evaluation\t0.246 \t\t0.518 \t\t1.491\n",
      "SARSA\t\t0.0 \t\t1.699 \t\t59.868\n",
      "QLearning\t2.058 \t\t2.209 \t\t85.425\n",
      "\n",
      "========\n",
      "Horse shoe\n",
      "========\n",
      "Path Length\tGreedy\t\tEpsilon\t\tExplorative\n",
      "------------------------------------------------------------\n",
      "Evaluation\t16 \t\t22 \t\t4906\n",
      "SARSA\t\t0 \t\t22 \t\t3141\n",
      "QLearning\t0 \t\t0 \t\t0\n",
      "\n",
      "Time\t\tGreedy\t\tEpsilon\t\tExplorative\n",
      "------------------------------------------------------------\n",
      "Evaluation\t0.27 \t\t0.407 \t\t1.628\n",
      "SARSA\t\t0.0 \t\t1.775 \t\t205.189\n",
      "QLearning\t0.0 \t\t0.0 \t\t0.0\n",
      "\n",
      "========\n",
      "Minefield\n",
      "========\n",
      "Path Length\tGreedy\t\tEpsilon\t\tExplorative\n",
      "------------------------------------------------------------\n",
      "Evaluation\t23 \t\t29 \t\t14462\n",
      "SARSA\t\t0 \t\t422 \t\t0\n",
      "QLearning\t0 \t\t0 \t\t0\n",
      "\n",
      "Time\t\tGreedy\t\tEpsilon\t\tExplorative\n",
      "------------------------------------------------------------\n",
      "Evaluation\t1.18 \t\t1.7 \t\t4.481\n",
      "SARSA\t\t0.0 \t\t4.913 \t\t0.0\n",
      "QLearning\t0.0 \t\t0.0 \t\t0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapname = [\"Shield\", \"Horse shoe\", \"Minefield\"]\n",
    "\n",
    "for i, methodScores in enumerate(collection):\n",
    "    \n",
    "    print(\"========\")\n",
    "    print(mapname[i])\n",
    "    print(\"========\")\n",
    "    print(\"Path Length\\tGreedy\\t\\tEpsilon\\t\\tExplorative\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Evaluation\\t\"+str(methodScores[0][0][1]), \"\\t\\t\"+str(methodScores[0][1][1]), \"\\t\\t\"+str(methodScores[0][2][1]))\n",
    "    print(\"SARSA\\t\\t\"+str(methodScores[1][0][1]), \"\\t\\t\"+str(methodScores[1][1][1]), \"\\t\\t\"+str(methodScores[1][2][1]))\n",
    "    print(\"QLearning\\t\"+str(methodScores[2][0][1]), \"\\t\\t\"+str(methodScores[2][1][1]), \"\\t\\t\"+str(methodScores[2][2][1]))\n",
    "    print()\n",
    "    \n",
    "    print(\"Time\\t\\tGreedy\\t\\tEpsilon\\t\\tExplorative\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Evaluation\\t\"+str(round(methodScores[0][0][0],3)), \"\\t\\t\"+str(round(methodScores[0][1][0],3)), \"\\t\\t\"+str(round(methodScores[0][2][0],3)))\n",
    "    print(\"SARSA\\t\\t\"+str(round(methodScores[1][0][0],3)), \"\\t\\t\"+str(round(methodScores[1][1][0],3)), \"\\t\\t\"+str(round(methodScores[1][2][0],3)))\n",
    "    print(\"QLearning\\t\"+str(round(methodScores[2][0][0],3)), \"\\t\\t\"+str(round(methodScores[2][1][0],3)), \"\\t\\t\"+str(round(methodScores[2][2][0],3)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ended after 39 iterations\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |*| |*|*|*|*|*|*|*|*| | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |*|*|*| |X| | | | |*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| |S|*|*| | | |X| | | | | |G| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "Process ended after 28 iterations\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|*| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| | | |*| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |S| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| | | |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X|X|X|X|X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|G| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "\n",
      "Process ended after 39 iterations\n",
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | |X|X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "|X|X|S| | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | |*| | | | | | | | |X| |X| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | |*|*|X| | |X| | |X| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |*|*| | |X|X| | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | |*| | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | |*| | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | |*| |X| | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| |X| | |*| | | | |X| | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | |X| |*|*| | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | |*|*|*| | |X| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | |*|*|*| | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X| | | | | |*|*|*|*|G| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | |X| | |X| | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = policyEvaluation(shield, EpsilonGreedyPolicy(shield, epsilon=0.1, optimiser=\"V\"), show=True)\n",
    "print(shield.print_solution(policy.run()))\n",
    "policy = policyEvaluation(horseShoe, EpsilonGreedyPolicy(horseShoe, epsilon=0.1, optimiser=\"V\"), show=True)\n",
    "print(horseShoe.print_solution(policy.run()))\n",
    "policy = policyEvaluation(minefield, EpsilonGreedyPolicy(minefield, epsilon=0.1, optimiser=\"V\"), show=True)\n",
    "print(minefield.print_solution(policy.run()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | |*|*|*|*|*|*| | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | |*|*|*| | |X| |*| | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| |S|*| | | | |X| |*|*|*|*|G| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | |X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = SARSA(shield, EpsilonGreedyPolicy(shield, epsilon=0.1, optimiser=\"Q\"), episodes=500)\n",
    "print(shield.print_solution(policy.run()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| |*|*|*|*| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*|*| | |*| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |S| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X| |*| |X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*| |X|X|X|X|X| | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|G| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = SARSA(horseShoe, EpsilonGreedyPolicy(horseShoe, epsilon=0.1, optimiser=\"Q\"), episodes=500)\n",
    "print(horseShoe.print_solution(policy.run()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution:\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|*| | | | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|*|*|X|X| | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "|X|X|S|*|*|*|*| | | | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | |*| |*| |*| | | | |X| |X| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| |*|*| |X|*|*|X| | |X| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*| | |*|*|*|X|X| | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "|*|*|*|*|*|*|*|*|*| | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | |*|*|*| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | |X| | |*|*| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| |X| | | | | | | |X|*| | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | |X| | | | | | | |*|*|*|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | |X| |X|*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | | | | | | | | | |*| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | |X| | | | | | | | | |G| |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "| | | | | |X| | |X| | | | | | |\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = SARSA(minefield, EpsilonGreedyPolicy(minefield, epsilon=0.1, optimiser=\"Q\"), episodes=500)\n",
    "print(minefield.print_solution(policy.run()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the various methods, it is easy to see that policy evaluation iteration provides the best results in the shortest time with the greatest flexibility. Through its operation, it focuses attention on perfecting the V function for the states of the environments with respect to the policies view. The V function for a particular policy tends to push an agent towards a set of actions for a particular environment, making differing policies have similar characteristics. It does, however, seem to suffer from more explorative/random approaches, as it cannot overcompensate for actions.\n",
    "\n",
    "This is leaps and bounds above the method Q Learning, that within 500 episodes was very slow and unoptimal for the easiest case and unfeasible for the remaining Grid worlds. What was surprising was its combination with the explorative policy. As Q-learning determines the value for state/actions in an expression that contains the best future action rather than the future action of the policy, it's behaviour becomes more evident. The Random explorative nature of the policy is driven by the value deltas of the adjacent states, and since the value is being determined quite accurately without the impact of the explorative randomness. It better leads the agent to the goal. Equally, the randomness in the policy helps to ensure that it traverses lots of the space without getting stuck.\n",
    "\n",
    "The SARSA method was relatively promising, but it is definitely suited to having a small element of randomness in its traversal. The greedy policy continued to lead the agent into a loop and due to its lack of compromise, that meant that such a combination was never viable. Furthermore, Its large time required when using the explorative policy indicates that the epsilon-greedy approach with an epsilon value around 0.1-0.2 is likely best. With that, the method gets very good near-optimal results.\n",
    "\n",
    "SARSA and Q-Learning definitely seem to have some advantages over the other when considering the type of problem that is being faced. Q-Learning is able to generate more optimal paths for lower iterations on simplistic Grids. A by-product of its value only look ahead, it is risky, and this benefits it on simple courses. Whereas SARSA can produce very optimal results if given enough time to solve a problem, and for difficult tasks, is much better and navigating them in quicker time with a near-optimal route. This is because SARSA better learns to avoid cliffs through its look ahead expression including the policies aspects, where the Q-learning approach would never consider them and fall in. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
